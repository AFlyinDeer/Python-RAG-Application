# PDF RAG (Retrieval-Augmented Generation) System
A comprehensive document question-answering system that processes PDF and DOCX files using ChromaDB vector storage and local LLM inference with Ollama.

# ğŸš€ Features
Multi-format Support: Process both PDF and DOCX documents \
Local LLM Integration: Uses Ollama with Llama 3.1 for privacy-focused inference \
Vector Database: ChromaDB for efficient similarity search and document retrieval \
Incremental Updates: Add new documents without rebuilding the entire database \
Interactive Q&A: Command-line interface for real-time document querying \
Similarity Filtering: Advanced retrieval with configurable similarity thresholds \
Source Attribution: Automatic citation of source documents and page numbers \
Automatic Setup: Database initialization with error handling and status checking

# ğŸ“‹ Prerequisites
Required Software \
Python 3.8+ \
Ollama installed and running locally \
Required Python packages (see Installation section) \
Ollama Models

# ğŸ¦™ğŸ§  Install the LLM model (instruct = GPU Optimized models)
ollama pull llama3.1 \
ollama run llama3.2:1b-instruct-q4_0 \
ollama run llama3.2:3b-instruct-q4_0


# ğŸ¦™ğŸ”¢ Install the embedding model (Light, Medium, Heavy)
ollama pull all-minilm \
ollama pull nomic-embed-text \
ollama pull mxbai-embed-large

# ğŸ Installation
Clone or download the project files \
Install Python dependencies: \
In a virtual environment run:

py -3 -m venv .venv  \
.venv \Scripts \activate

pip install -r requirements.txt

# âš›ï¸ Setting up React/Node.js
Install React dependencies: \
cd frontend

Intall Node.js dependencies: \
npm install


# ğŸš€ Quick Start ğŸš€
1. Add Your Documents \
Place your PDF and DOCX files in the documents/ directory:

documents/ \
â”œâ”€â”€ research_paper.pdf \
â”œâ”€â”€ manual.docx \
â””â”€â”€ report.pdf

# â–¶ï¸ First, set up the database
python db_setup.py

# ğŸ“Ÿ Terminal 1 - Start the backend
python server.py

# ğŸ“Ÿ Terminal 2 - Start the frontend  
cd frontend \
npm start

# ğŸŒ Access the application at 
http://localhost:5000

# âš™ï¸ Configuration
Found in config.py

# ğŸ”§ Advanced Features
Adding New Documents \
The system automatically detects new files

# ğŸ“‚ Add new files to docs/ directory, then run:
python db_setup.py


# ğŸ“Š Performance Optimization
Memory Management \
Uses garbage collection (gc.collect()) after processing \
Processes documents in configurable batches \
Efficient text splitting with overlap \
Speed Optimization \
ChromaDB for fast vector similarity search \
Configurable similarity thresholds \
Optimized LLM parameters for faster inference \
Storage Efficiency \
Incremental database updates \
Persistent vector storage \
Metadata-based file tracking

# ğŸ”’ Privacy & Security
Fully Local: No data sent to external services \
Ollama Integration: Local LLM inference only \
Document Privacy: Files processed and stored locally \
No API Keys: No external API dependencies \

# ğŸ“„ License
This project is provided as-is for educational and research purposes.

# ğŸ†˜ Support
For issues and questions:

Check the troubleshooting section above \
Verify all dependencies are installed correctly \
Ensure Ollama is running with required models \
Check file paths and permissions \
Note: Make sure to adjust the file paths in the configuration files to match your system setup before running the application.

